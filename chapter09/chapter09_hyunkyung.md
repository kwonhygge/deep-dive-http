# 9장 웹 로봇

- 웹 로봇은 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램
## 9.1 크롤러와 크롤링
- 웹 크롤러는, 먼저 웹페이지를 한 개 가져오고, 그 다음 그 페이지가 가리키는 모든 웹페이지를 가져오고, 다시 그 페이지들이 가리키는 모든 웹페이지들을 가져오는
그런 일을 재귀적으로 반복하는 방식으로 웹을 순회하는 로봇
- HTML 하이퍼링크들로 만들어진 웹을 따라 '기어다니기' 때문에 크롤러 혹은 스파이더라고 불림
### 9.1.1 어디에서 시작하는가: '루트 집합'
- 크롤러가 방문을 시작하는 URL들의 초기 집합은 루트 집합이라고 불림
- 루트 집합을 고를 때, 모든 링크를 크롤링하면 결과적으로 관심 있는 웹페이지들의 대부분을 가져올 수 있도록 충분히 다른 장소에서 URL들을 선택해야 함
- 웹의 대부분을 커버하기 위해 루트 집합에 너무 많은 페이지가 있을 필요는 없음
- 일반적으로 좋은 루트 집합은 크고 인기 있는 웹 사이트, 새로 생성된 페이지들의 목록, 그리고 자주 링크되지 않는 잘 알려져 있지 않은 페이지들의 목록으로 구성되어 있음
- 루트 집합은 시간이 지남에 따라 성장하며 새로운 크롤링을 위한 시드 목록이 됨
### 9.1.2 링크 추출과 상대 링크 정상화
- 크롤러는 웹을 돌아다니면서 꾸준히 HTML 문서를 검색
- 크롤러는 검색한 각 페이지 안에 들어있는 URL 링크들을 파싱해서 크롤링할 페이지들의 목록에 추가해야 함
- 이 목록은 보통 급속히 확장됨
- 크롤러들은 간단한 HTML 파싱을 해서 이들 링크들을 추출하고 상대 링크를 절대 링크로 변환할 필요가 있음
### 9.1.3 순환 피하기
- 로봇이 웹을 크롤링 할 때, 루프나 순환에 빠지지 않도록 매우 조심해야 함
- 로봇들은 순환을 피하기 위해 반드시 그들이 어디를 방문했는지 알아야 함
### 9.1.4 루프와 중복
- 순환이 해로운 세 가지 이유
1. 순환은 크롤러를 루프에 빠뜨려서 꼼짝 못하게 만들 수 있음, 네트워크 대역폭을 다 차지하고 그 어떤 페이지도 가져올 수 없게 되어버릴 수도
2. 웹 서버의 부담이 됨, 실제 사용자도 사이트에 접근할 수 없을 수도
3. 크롤러는 많은 수의 중복된 페이지들을 가져오게 되고 쓸모없는 콘텐츠로 넘쳐나게 될 것
### 9.1.5 빵 부스러기의 흔적
- 방문한 곳을 지속적으로 추적하는 것은 쉽지 않음
- 복잡한 자료구조를 사용하여 속도와 메모리 사용 면에서 효과적이어야 함
- 빠른 검색 구조를 요구하기 때문에 빠른 속도는 중요함
- 로봇은 적어도 검색 트리나 해시 테이블을 필요로 할 것
- 많은 공간을 차지하기도 함
- 유용한 기법
    - 트리와 해시 테이블: 빠르게 찾아볼 수 있게 해주는 소프트웨어 자료 구조
    - 느슨한 존재 비트맵: 공간 사용을 최소화하기 위해, 존재 비트 배열과 같은 느슨한 자료구조 사용
        - 각 URL은 해시 함수에 의해 고정된 크기의 숫자로 변환되고 배열 안에 대응하는 '존재 비트'를 가짐
        - URL이 크롤링 되었을 때, 해당하는 존재 비트가 만들어짐, 만약 존재 비트가 이미 존재한다면, 크롤러는 그 URL을 이미 크롤링 되었다고 간주
    - 체크포인트: 로봇 프로그램이 갑작스럽게 중단될 경우를 대비해, 방문한 URL의 목록이 디스크에 저장되었는지 확인
    - 파티셔닝
        - 한 대의 컴퓨터에서 하나의 로봇이 크롤링을 완수하는 것은 불가능해짐
        - 대규모 웹 로봇은 각각이 분리된 한 대의 컴퓨터인 로봇들이 동시에 일하고 있는 농장(farm)을 이용
        - 각 로봇엔 URL들의 특정 '한 부분'이 할당되어 그에 대한 책임을 짐
        - 서로 도와 웹을 크롤링하며 커뮤니케이션을 함
### 9.1.6 별칭(alias)과 로봇 순환
- 올바른 자료구조를 갖추었더라도 URL이 별칭을 가질 수 있는 이상 어떤 페이지를 이전에 방문했었는지 말해주기 어려움
- 한 URL이 또 다른 URL에 대한 별칭이라면, 그 둘이 서로 달라 보이더라도 사실은 같은 리소스를 가리키고 있음
### 9.1.7 URL 정규화하기
- URL들을 표준 형식으로 '정규화' 함으로써 다른 URL과 같은 리소스를 가리키고 있음이 확실한 것들을 미리 제거하려 시도
- 변환 방법
    - 포트 번호가 명시되지 않았다면, 호스트 명에 ':80'을 추가
    - 모든 %xx 이스케이핑된 문자들을 대응되는 문자로 변환
    - # 태그들을 제거
- 웹 서버에 대한 지식 필요
    - 웹 서버가 대소문자를 구분한다는 것
    - 디렉터리에 대한 웹 서버의 색인 페이지 설정을 알 필요가 있을 것
    - IP 주소가 어디를 참조하는지, 웹 서버가 가상 호스팅을 하도록 설정되어 있는지도 알아야 함
- URL 정규화는 기본적인 문법의 별칭을 제거할 수 있지만, 로봇들은 URL을 표준 형식으로 변환하는 것만으로는 제거할 수 없는 다른 URL 별칭도 만나게 될 것
### 9.1.8 파일 시스템 링크 순환
- 파일 시스템의 심벌릭 링크는 사실상 아무것도 존재하지 않으면서도 끝없이 깊어지는 디렉터리 계층을 만들 수 있기 때문에, 매우 교묘한 종류의 순환을 유발할 수 있음
- 서버 관리자가 실수로 만들거나 사악한 웹 마스터가 악의적으로 만들기도 함
### 9.1.9 동적 가상 웹 공간
- 게이트웨이 애플리케이션인 URL을 만들어 무한한 가상 웹 공간으로 크롤러를 보내버릴 수도
- 혹은 우연히 만들어질 수도 - 한달 치 달력을 생성하고 그 다음달로 링크를 걸어주는 달력 프로그램 같이
### 9.1.10 루프와 중복 피하기
- 문제를 피하는데 더 적합한 로봇은 의심스러워 보이지만 실은 유효한 콘텐츠를 걸러버릴 수도 있음
- 로봇이 더 올바르게 동작하기 위해 사용되는 기법들
- URL 정규화: URL을 표준 형태로 변환함으로써, 같은 리소스를 가리키는 중복된 URL이 생기는 것을 일부 회피
- 너비 우선 크롤링: 방문할 URL들을 너비 우선으로 스케줄링하면, 순환의 영향을 최소화할 수 있음
- 스로틀링: 로봇이 웹 사이트에서 일정 시간 동안 가져올 수 있는 페이지의 숫자를 제한함
- URL 크기 제한: 로봇은 일정 길이를 넘는 URL의 크롤링은 거부할 수 있음, 가져오지 못하는 콘텐츠가 있다는 것이 단점
- URL/사이트 블랙리스트: 로봇 순환을 만들어 내거나 함정인 사이트와 URL의 목록을 만들어 관리
- 패턴 발견: 순환을 만드는 사이트들은 일정 패턴을 따르는 경향이 있음
- 콘텐츠 지문: 페이지의 콘텐츠에서 몇 바이트를 얻어내어 체크섬을 계산 -> 그 페이지 내용의 간략한 표현, 동적인 동작은 중복 감지를 방해함
- 사람의 모니터링

## 9.2 로봇의 HTTP
- 로봇들 또한 HTTP 명세의 규칙을 지켜야 함
### 9.2.1 요청 헤더 식별하기
- 그들 대부분은 약간의 신원 식별 헤더를 구현하고 전송함
- User-Agent: 서버에게 요청을 만든 로봇의 이름을 말해줌
- From: 로봇의 사용자/관리자의 이메일 주소를 제공
- Accept: 서버에게 어떤 미디어 타입을 보내도 되는지 말해줌
- Referer: 현재의 요청 URL을 포함한 문서의 URL을 제공
### 9.2.2 가상 호스팅
- 가상 호스팅이 널리 퍼져있는 현실에서 요청에 Host 헤더를 포함하지 않으면 로봇이 어떤 URL에 대해 잘못된 콘텐츨를 찾게 만듦
### 9.2.3 조건부 요청
- 로봇이 검색하는 콘텐츠의 양을 최소화하는 것은 상당히 의미 있는 일
- 이들 로봇 중의 몇몇은 시간이나 엔터티 태그를 비교함으로써 그들이 받아간 마지막 버전 이후에 업데이트 된 것이 있는지 알아보는 조건부 HTTP 요청을 구현함
### 9.2.4 응답 다루기
- 대다수 로봇들은 주 관심사가 단순히 GET 메서드로 콘텐츠를 요청해서 가져오는 것이기 때문에, 응답 다루기라고 부를 만한 일은 거의 하지 않음
- 그러나 웹 탐색이나 서버와의 상호작용을 더 잘해보려고 하는 로봇들은 여러 종류의 응답을 다룰 줄 알 필요가 있음
- 상태 코드
    - 일반적인 상태 코드나 예상할 수 있는 상태 코드를 다룰 수 있어야 함
- 엔터티
    - HTTP 헤더에 임베딩된 정보를 따라 로봇들은 엔터티 자체의 정보를 찾을 수 있음
### 9.2.5 User-Agent 타기팅
- 웹 관리자들은 많은 로봇이 그들의 사이트를 방문하게 될 것임을 명심하고, 그 로봇들로부터의 요청을 예상해야 함
- 많은 웹 사이트들은 브라우저의 종류를 감지하여 그에 맞게 콘텐츠를 최적화함
- 이것을 함으로써 로봇에게 콘텐츠 대신 에러 페이지를 제공
- 사이트 관리자들은 최소한 로봇이 그들의 사이트에 방문했다가 콘텐츠를 얻을 수 없어 당황하는 일이 없도록 대비해야 함

## 9.3 부적절하게 동작하는 로봇들
- 폭주하는 로봇
    - 만약 로봇이 논리적인 에러를 갖고 있거나 순환에 빠졌다면 웹 서버에 극심한 부하를 안겨줄 수도 있음
- 오래된 URL: 존재하지 않는 문서에 대한 접근 요청으로 에러 로그가 채워지거나, 웹 서버의 요청에 대한 수용 능력이 감소됨
- 길고 잘못된 URL: 웹 서버의 처리 능력에 영향을 주고, 웹 서버의 접근 로그를 어지럽게 채우고, 고장을 일으킬 위험도 있음
- 호기심이 지나친 로봇: 민감한 데이터를 검색하여 사생활 침해를 일으킬수도
- 동적 게이트웨이 접근: 로봇은 게이트웨이 애플리케이션의 콘텐츠에 대한 URL로 요청을 할 수도 있고 이렇게 얻은 데이터는 처리 비용이 많이 들 것









